{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5308e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1a0b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transition matrix and dataset\n",
    "P_df = pd.read_csv(\"./dataset/P.csv\", index_col=0).values\n",
    "ts_df = pd.read_csv(\"./dataset/generated_data/ts_df.csv\")\n",
    "\n",
    "\n",
    "# Reshape into 3 action-specific 4x4 matrices\n",
    "P = [P_df[i*4:(i+1)*4, :] for i in range(3)]  # P[0], P[1], P[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e1a9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reward function\n",
    "def Reward(a, b, mode):\n",
    "    R = np.zeros((4, 3))\n",
    "    if mode == \"reward\":\n",
    "        R[0, :] += 1\n",
    "    else:\n",
    "        R[3, :] -= 1\n",
    "    R[:, 1] -= a\n",
    "    R[:, 2] -= b\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93345100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Value Iteration with Q(s,a)\n",
    "# ------------------------\n",
    "def value_iteration_with_q(P, R, gamma=0.95, theta=1e-4):\n",
    "    Q = np.zeros((4, 3))  # Q(s, a)\n",
    "\n",
    "    for _ in range(1000):\n",
    "        delta = 0\n",
    "        Q_new = np.copy(Q)\n",
    "        for s in range(4):\n",
    "            for a in range(3):\n",
    "                Q_new[s, a] = R[s, a] + gamma * np.dot(P[a][s], np.max(Q, axis=1))\n",
    "                delta = max(delta, abs(Q_new[s, a] - Q[s, a]))\n",
    "        Q = Q_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    V = np.max(Q, axis=1)\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    return Q, V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c7fecc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, R, ts_df):\n",
    "    rewards = []\n",
    "    current_reward = 0\n",
    "\n",
    "    for _, row in ts_df.iterrows():\n",
    "                \n",
    "        s = int(row['cluster'])\n",
    "        a = policy[s]\n",
    "        current_reward += R[s, a]\n",
    "\n",
    "        # Check if this row ends a trajectory\n",
    "        if np.isnan(row['cluster_n']):\n",
    "            rewards.append(current_reward)\n",
    "            current_reward = 0\n",
    "\n",
    "    # Catch any last incomplete trajectory\n",
    "    if current_reward > 0:\n",
    "        rewards.append(current_reward)\n",
    "\n",
    "    # Print and return mean\n",
    "    mean_reward = np.mean(rewards)\n",
    "    print(f\"Mean Accumulated Reward per Trajectory: {mean_reward:.3f}\")\n",
    "    return mean_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5071d15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accumulated Reward per Trajectory: 1.758\n",
      "Mean Accumulated Reward per Trajectory: -1.647\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Run for both modes\n",
    "# ------------------------\n",
    "\n",
    "a, b = 0.01, 0.025\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "results = {}\n",
    "\n",
    "for mode in [\"reward\", \"penalty\"]:\n",
    "    R = Reward(a, b, mode)\n",
    "    Q, V, policy = value_iteration_with_q(P, R, gamma=gamma)\n",
    "    mean_reward = evaluate_policy(policy, R, ts_df)\n",
    "\n",
    "    results[mode] = {\n",
    "        \"Q\": Q,\n",
    "        \"V\": V,\n",
    "        \"policy\": policy,\n",
    "        \"mean_reward\": mean_reward\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a4103c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== REWARD MODE ======\n",
      "Q-function (state-action values):\n",
      "[[7.556 7.529 7.597]\n",
      " [6.364 6.371 6.258]\n",
      " [6.389 6.38  6.427]\n",
      " [6.33  6.311 6.322]]\n",
      "V-function (state values):\n",
      "[7.597 6.371 6.427 6.33 ]\n",
      "Policy (state → best action):\n",
      "  State 0 → Action 2\n",
      "  State 1 → Action 1\n",
      "  State 2 → Action 2\n",
      "  State 3 → Action 0\n",
      "Mean Reward: 1.758\n",
      "\n",
      "====== PENALTY MODE ======\n",
      "Q-function (state-action values):\n",
      "[[-3.69  -3.73  -3.685]\n",
      " [-3.776 -3.763 -3.852]\n",
      " [-3.694 -3.758 -3.748]\n",
      " [-4.899 -4.906 -5.006]]\n",
      "V-function (state values):\n",
      "[-3.685 -3.763 -3.694 -4.899]\n",
      "Policy (state → best action):\n",
      "  State 0 → Action 2\n",
      "  State 1 → Action 1\n",
      "  State 2 → Action 0\n",
      "  State 3 → Action 0\n",
      "Mean Reward: -1.647\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Print Results\n",
    "# ------------------------\n",
    "for mode in [\"reward\", \"penalty\"]:\n",
    "    print(f\"\\n====== {mode.upper()} MODE ======\")\n",
    "    print(\"Q-function (state-action values):\")\n",
    "    print(np.round(results[mode][\"Q\"], 3))\n",
    "    print(\"V-function (state values):\")\n",
    "    print(np.round(results[mode][\"V\"], 3))\n",
    "    print(\"Policy (state → best action):\")\n",
    "    for s in range(4):\n",
    "        print(f\"  State {s} → Action {results[mode]['policy'][s]}\")\n",
    "    print(f\"Mean Reward: {results[mode]['mean_reward']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
