{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 6. MDP model solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox, mdptoolbox.example\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read probability matrix from part 5\n",
    "ts_df = pd.read_csv(\"ts_df.csv\")\n",
    "ts_df = ts_df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.fillna(-1, inplace = True)\n",
    "ts_df['action'] = ts_df['action'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_CV(input, CV):\n",
    "    data_list = list(input)\n",
    "    random.shuffle(data_list)\n",
    "\n",
    "    subset_size = len(data_list) // CV\n",
    "    remainder = len(data_list) % CV\n",
    "\n",
    "    subsets = []\n",
    "    start_idx = 0\n",
    "    for i in range(CV):\n",
    "        end_idx = start_idx + subset_size + (1 if i < remainder else 0)\n",
    "        subsets.append(data_list[start_idx:end_idx])\n",
    "        start_idx = end_idx\n",
    "    return subsets\n",
    "\n",
    "#define the reward function\n",
    "def Reward(a, b, mode):\n",
    "    R = np.zeros(12).reshape(4,3)\n",
    "    if mode == \"reward\":\n",
    "        R[0,:] += 1\n",
    "    else:\n",
    "        R[3,:] -= 1\n",
    "    R[:,1] -= a\n",
    "    R[:,2] -= b\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward\n",
      "opti \t 0.3223 \t 0.0033\n",
      "worst \t 0.2938 \t 0.0043\n",
      "const \t 0.3113 \t 0.0079\n",
      "random \t 0.3243 \t 0.0107\n",
      "real \t 0.0407\n",
      "penalty\n",
      "opti \t -0.1478 \t 0.0051\n",
      "worst \t -0.1666 \t 0.0056\n",
      "const \t -0.1453 \t 0.0061\n",
      "random \t -0.1586 \t 0.0058\n",
      "real \t -0.1609\n"
     ]
    }
   ],
   "source": [
    "# divide the data into 1:9\n",
    "# calculate P\n",
    "# solve the MDP (for 2)\n",
    "# \n",
    "CV = 5\n",
    "\n",
    "patients = ts_df.iloc[:,0]\n",
    "patients = set(patients)\n",
    "subsets = divide_CV(patients, CV)\n",
    "\n",
    "tot_sim, tot_const, tot_random, tot_sim_w, tot_real = [[],[]], [[],[]], [[],[]], [[],[]], [[],[]]\n",
    "LEDD_total = [0 for _ in range(7)] #sim_r, sim_w, worst_p, worst_p, constant, random, actual \n",
    "LEDD_total = {\"optimal\":[[],[]],\n",
    "                \"random\":[[],[]],\n",
    "                \"const\":[[],[]],\n",
    "                \"worst\":[[],[]],\n",
    "                \"real\":[[],[]]}\n",
    "tests = [\"optimal\", \"random\", \"const\", \"worst\", \"real\"]\n",
    "\n",
    "for i in range(CV+1):\n",
    "    if i < CV:\n",
    "        valid_idx = subsets[i]\n",
    "        train_idx = []\n",
    "        for j in range(CV):\n",
    "            if i != j:\n",
    "                train_idx += subsets[j]\n",
    "\n",
    "        valid = ts_df[ts_df['PATNO'].isin(valid_idx)]\n",
    "        train = ts_df[ts_df['PATNO'].isin(train_idx)]\n",
    "    else:\n",
    "        valid = ts_df\n",
    "\n",
    "    #create probablity matrix P\n",
    "    P_t = np.zeros(4*3*4).reshape(3,4,4)\n",
    "    P_v = np.zeros(4*3*4).reshape(3,4,4)\n",
    "\n",
    "    for a in range(3):\n",
    "        for s in range(4):\n",
    "            for s_ in range(3):\n",
    "                criteria = ((train['cluster'] == s) & (train['action'] == a))\n",
    "                criteria_2 = ((train['cluster'] == s) & (train['action'] == a) & (train['cluster_n'] == s_))\n",
    "                P_t[a,s,s_] = criteria_2.sum()/criteria.sum()\n",
    "\n",
    "            P_t[a,s,3] = 1- sum(P_t[a,s,:])\n",
    "\n",
    "    for a in range(3):\n",
    "        for s in range(4):\n",
    "            for s_ in range(3):\n",
    "                criteria = ((ts_df['cluster'] == s) & (ts_df['action'] == a))\n",
    "                criteria_2 = ((ts_df['cluster'] == s) & (ts_df['action'] == a) & (ts_df['cluster_n'] == s_))\n",
    "                P_v[a,s,s_] = criteria_2.sum()/criteria.sum()\n",
    "\n",
    "            P_v[a,s,3] = 1 - sum(P_v[a,s,:])\n",
    "\n",
    "\n",
    "    a, b = 0.01, 0.025\n",
    "    R_r = Reward(a, b,\"reward\")\n",
    "    mdp_model_r = mdptoolbox.mdp.PolicyIteration(P_t, R_r, 0.99)\n",
    "    mdp_model_r.run()\n",
    "    # if i < CV:\n",
    "    #     print(f'CV = {i+1} / Reward Model / {mdp_model_r.policy}' , end = \"\")\n",
    "    # else:\n",
    "    #     print(f'Full D / Reward Model / {mdp_model_r.policy}' , end = \"\")\n",
    "\n",
    "    R_p = Reward(a, b,\"penalty\")\n",
    "    mdp_model_p = mdptoolbox.mdp.PolicyIteration(P_t, R_p, 0.99)\n",
    "    mdp_model_p.run()\n",
    "    #print(f' / Penalty Model / {mdp_model_p.policy}')\n",
    "\n",
    "    R_r_w = - R_r\n",
    "    mdp_model_r_w = mdptoolbox.mdp.PolicyIteration(P_t, R_r_w, 0.99)\n",
    "    mdp_model_r_w.run()\n",
    "\n",
    "    R_p_w = - R_p\n",
    "    mdp_model_p_w = mdptoolbox.mdp.PolicyIteration(P_t, R_p_w, 0.99)\n",
    "    mdp_model_p_w.run()\n",
    "\n",
    "    #policy = {\"r\":mdp_model_r.policy, \"p\":mdp_model_p.policy, \"r_w\" : mdp_model_r_w.policy, \"p_w\" : mdp_model_r_w.policy}\n",
    "    policy = {\"r\":mdp_model_r.policy, \"p\":mdp_model_p.policy, \"r_w\" : mdp_model_r_w.policy, \"p_w\" : mdp_model_r_w.policy}\n",
    "    sim_reward, random_reward, const_reward, sim_reward_w, real_reward = [0,0], [0,0], [0,0], [0,0], [0,0]\n",
    "\n",
    "    tot_n = 0\n",
    "    for pat in valid_idx:\n",
    "        pat_d = valid[valid['PATNO'] == pat]\n",
    "        n = len(pat_d)\n",
    "        init_LEDD = list(pat_d['LEDD'])[0]\n",
    "        LEDD = {\"optimal\":[init_LEDD,init_LEDD],\n",
    "                \"random\":[init_LEDD,init_LEDD],\n",
    "                \"const\":[init_LEDD,init_LEDD],\n",
    "                \"worst\":[init_LEDD,init_LEDD],\n",
    "                \"real\":[0,0]} #sim_r, sim_p, worst_r, worst_p, constant, random\n",
    "        LEDD_traj = {\"optimal\":[init_LEDD,init_LEDD],\n",
    "                \"random\":[init_LEDD,init_LEDD],\n",
    "                \"const\":[init_LEDD,init_LEDD],\n",
    "                \"worst\":[init_LEDD,init_LEDD],\n",
    "                \"real\":[0,0]}\n",
    "\n",
    "        state = pat_d['cluster']\n",
    "        state = [list(state)[0],list(state)[0]]\n",
    "        \n",
    "        LEDD_total[\"real\"].append(pat_d[\"LEDD\"].mean())\n",
    "\n",
    "        #simul\n",
    "        for t in tests:\n",
    "            if state[0] == 0:\n",
    "                if t == \"optimal\":\n",
    "                    sim_reward[0] += 1\n",
    "                elif t == \"const\":\n",
    "                    const_reward[0] += 1\n",
    "                elif t == \"worst\":\n",
    "                    sim_reward_w[0] += 1\n",
    "                elif t == \"real\":\n",
    "                    real_reward[0] += 1\n",
    "                else:\n",
    "                    random_reward[0] += 1\n",
    "            elif state[1] == 3:\n",
    "                if t == \"optimal\":\n",
    "                    sim_reward[1] -= 1\n",
    "                elif t == \"const\":\n",
    "                    const_reward[1] -= 1\n",
    "                elif t == \"worst\":\n",
    "                    sim_reward_w[1] -= 1\n",
    "                elif t == \"real\":\n",
    "                    real_reward[1] -= 1\n",
    "                else:\n",
    "                    random_reward[1] -= 1\n",
    "\n",
    "            for nn in range(n-1): \n",
    "            \n",
    "                if t == \"optimal\":\n",
    "                    action = [policy['r'][state[0]], policy['p'][state[1]]]\n",
    "                elif t == \"const\":\n",
    "                    action = [0,0]\n",
    "                elif t == \"worst\":\n",
    "                    action = [policy[\"r_w\"][state[1]], policy[\"p_w\"][state[1]]]\n",
    "                elif t == \"real\":\n",
    "                    action = [list(pat_d[\"action\"])[nn],list(pat_d[\"action\"])[nn]]\n",
    "                else:\n",
    "                    action = [random.choice(range(3)), random.choice(range(3))]    \n",
    "                if action[0] == 1:\n",
    "                    LEDD[t][0] += 35\n",
    "                elif action[0] == 2:\n",
    "                    LEDD[t][0] += 180\n",
    "                else:\n",
    "                    if t != \"const\":\n",
    "                        LEDD[t][0] -= 28\n",
    "\n",
    "                if action[1] == 1:\n",
    "                    LEDD[t][1] += 35\n",
    "                elif action[1] == 2:\n",
    "                    LEDD[t][1] += 180\n",
    "                else:\n",
    "                    if t != \"const\":\n",
    "                        LEDD[t][1] -= 28\n",
    "                \n",
    "                \n",
    "                LEDD_traj[t][0] += LEDD[t][0]\n",
    "                LEDD_traj[t][1] += LEDD[t][1]\n",
    "\n",
    "                state_ = [random.choices(range(4), weights = P_v[action[0],state[0],:])[0], random.choices(range(4), weights = P_v[action[1],state[1],:])[0]]\n",
    "\n",
    "                for k in range(2):\n",
    "                    if action[k] == 1:\n",
    "                        if t == \"optimal\":\n",
    "                            sim_reward[k] -= 0.01\n",
    "                        elif t == \"const\":\n",
    "                            const_reward[k] -= 0.01\n",
    "                        elif t == \"worst\":\n",
    "                            sim_reward_w[k] -= 0.01\n",
    "                        elif t == \"real\":\n",
    "                            real_reward[k] -= 0.025\n",
    "                        else:\n",
    "                            random_reward[k] -= 0.01\n",
    "                    elif action[k] == 2:\n",
    "                        if t == \"optimal\":\n",
    "                            sim_reward[k] -= 0.025\n",
    "                        elif t == \"const\":\n",
    "                            const_reward[k] -= 0.025\n",
    "                        elif t == \"worst\":\n",
    "                            sim_reward_w[k] -= 0.025\n",
    "                        elif t == \"real\":\n",
    "                            real_reward[k] -= 0.025\n",
    "                        else:\n",
    "                            random_reward[k] -= 0.025\n",
    "                if state[0] == 0:\n",
    "                    if t == \"optimal\":\n",
    "                        sim_reward[0] += 1\n",
    "                    elif t == \"const\":\n",
    "                        const_reward[0] += 1\n",
    "                    elif t == \"worst\":\n",
    "                        sim_reward_w[0] += 1\n",
    "                    elif t == \"real\":\n",
    "                        real_reward[0] += 0.025\n",
    "                    else:\n",
    "                        random_reward[0] += 1\n",
    "                elif state[1] == 3:\n",
    "                    if t == \"optimal\":\n",
    "                        sim_reward[1] -= 1\n",
    "                    elif t == \"const\":\n",
    "                        const_reward[1] -= 1\n",
    "                    elif t == \"worst\":\n",
    "                        sim_reward_w[1] -= 1\n",
    "                    elif t == \"real\":\n",
    "                        real_reward[1] -= 1\n",
    "                    else:\n",
    "                        random_reward[1] -= 1\n",
    "                state = state_\n",
    "            \n",
    "            LEDD_traj[t][0] /= n\n",
    "            LEDD_traj[t][1] /= n\n",
    "\n",
    "            LEDD_total[t][0].append(LEDD_traj[t][0])\n",
    "            LEDD_total[t][1].append(LEDD_traj[t][1])\n",
    "\n",
    "        tot_n += n\n",
    "\n",
    "\n",
    "    sim_reward = [s/tot_n for s in sim_reward]\n",
    "    sim_reward_w = [s/tot_n for s in sim_reward_w]\n",
    "    const_reward = [s/tot_n for s in const_reward]\n",
    "    random_reward = [s/tot_n for s in random_reward]\n",
    "    real_rewards = [s/tot_n for s in real_reward]\n",
    "\n",
    "    for kk in range(2):\n",
    "        tot_sim[kk].append(sim_reward[kk])\n",
    "        tot_const[kk].append(const_reward[kk])\n",
    "        tot_random[kk].append(random_reward[kk])\n",
    "        tot_sim_w[kk].append(sim_reward_w[kk])\n",
    "        tot_real[kk].append(real_rewards[kk])\n",
    "\n",
    "tot1, tot2 = [], []\n",
    "#Real total\n",
    "\n",
    "#LEDD_total[\"real\"] = np.mean(LEDD_total[\"real\"])\n",
    "\n",
    "for t in tests:\n",
    "    LEDD_total[t][0] = np.mean(LEDD_total[t][0])\n",
    "    LEDD_total[t][1] = np.mean(LEDD_total[t][1])\n",
    "\n",
    "modes = [\"reward\", \"penalty\"]\n",
    "for aa in range(2):\n",
    "    print(modes[aa])\n",
    "    print(\"opti\",  \"\\t\", np.round(np.mean(tot_sim[aa]),4), \"\\t\", np.round(np.std(tot_sim[aa])/sqrt(CV),4))\n",
    "    print(\"worst\", \"\\t\", np.round(np.mean(tot_sim_w[aa]),4), \"\\t\", np.round(np.std(tot_sim_w[aa])/sqrt(CV),4))\n",
    "    print(\"const\",  \"\\t\", np.round(np.mean(tot_const[aa]),4),  \"\\t\", np.round(np.std(tot_const[aa])/sqrt(CV),4))\n",
    "    print(\"random\",  \"\\t\", np.round(np.mean(tot_random[aa]),4), \"\\t\", np.round(np.std(tot_random[aa])/sqrt(CV),4))\n",
    "    print(\"real \\t\", np.round(np.mean(tot_real[aa]),4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward\n",
      "opti \t 0.3122 \t 0.0087\n",
      "worst \t 0.3029 \t 0.0129\n",
      "const \t 0.309 \t 0.0057\n",
      "random \t 0.305 \t 0.0093\n",
      "real \t 0.0421\n",
      "penalty\n",
      "opti \t -0.1448 \t 0.0044\n",
      "worst \t -0.1593 \t 0.0047\n",
      "const \t -0.154 \t 0.0056\n",
      "random \t -0.1542 \t 0.008\n",
      "real \t -0.1692\n"
     ]
    }
   ],
   "source": [
    "# divide the data into 1:9\n",
    "# calculate P\n",
    "# solve the MDP (for 2)\n",
    "# \n",
    "CV = 10\n",
    "\n",
    "patients = ts_df.iloc[:,0]\n",
    "patients = set(patients)\n",
    "subsets = divide_CV(patients, CV)\n",
    "\n",
    "tot_sim, tot_const, tot_random, tot_sim_w, tot_real = [[],[]], [[],[]], [[],[]], [[],[]], [[],[]]\n",
    "LEDD_total = [0 for _ in range(7)] #sim_r, sim_w, worst_p, worst_p, constant, random, actual \n",
    "LEDD_total = {\"optimal\":[[],[]],\n",
    "                \"random\":[[],[]],\n",
    "                \"const\":[[],[]],\n",
    "                \"worst\":[[],[]],\n",
    "                \"real\":[[],[]]}\n",
    "tests = [\"optimal\", \"random\", \"const\", \"worst\", \"real\"]\n",
    "\n",
    "for i in range(CV+1):\n",
    "    for _ in range(1):\n",
    "        if i < CV:\n",
    "            valid_idx = subsets[i]\n",
    "            train_idx = []\n",
    "            for j in range(CV):\n",
    "                if i != j:\n",
    "                    train_idx += subsets[j]\n",
    "\n",
    "            valid = ts_df[ts_df['PATNO'].isin(valid_idx)]\n",
    "            train = ts_df[ts_df['PATNO'].isin(train_idx)]\n",
    "        else:\n",
    "            valid = ts_df\n",
    "\n",
    "        #create probablity matrix P\n",
    "        P_t = np.zeros(4*3*4).reshape(3,4,4)\n",
    "        P_v = np.zeros(4*3*4).reshape(3,4,4)\n",
    "\n",
    "        for a in range(3):\n",
    "            for s in range(4):\n",
    "                for s_ in range(3):\n",
    "                    criteria = ((train['cluster'] == s) & (train['action'] == a))\n",
    "                    criteria_2 = ((train['cluster'] == s) & (train['action'] == a) & (train['cluster_n'] == s_))\n",
    "                    P_t[a,s,s_] = criteria_2.sum()/criteria.sum()\n",
    "\n",
    "                P_t[a,s,3] = 1- sum(P_t[a,s,:])\n",
    "\n",
    "        for a in range(3):\n",
    "            for s in range(4):\n",
    "                for s_ in range(3):\n",
    "                    criteria = ((ts_df['cluster'] == s) & (ts_df['action'] == a))\n",
    "                    criteria_2 = ((ts_df['cluster'] == s) & (ts_df['action'] == a) & (ts_df['cluster_n'] == s_))\n",
    "                    P_v[a,s,s_] = criteria_2.sum()/criteria.sum()\n",
    "\n",
    "                P_v[a,s,3] = 1 - sum(P_v[a,s,:])\n",
    "\n",
    "\n",
    "        a, b = 0.01, 0.025\n",
    "        R_r = Reward(a, b,\"reward\")\n",
    "        mdp_model_r = mdptoolbox.mdp.PolicyIteration(P_t, R_r, 0.99)\n",
    "        mdp_model_r.run()\n",
    "        # if i < CV:\n",
    "        #     print(f'CV = {i+1} / Reward Model / {mdp_model_r.policy}' , end = \"\")\n",
    "        # else:\n",
    "        #     print(f'Full D / Reward Model / {mdp_model_r.policy}' , end = \"\")\n",
    "\n",
    "        R_p = Reward(a, b,\"penalty\")\n",
    "        mdp_model_p = mdptoolbox.mdp.PolicyIteration(P_t, R_p, 0.99)\n",
    "        mdp_model_p.run()\n",
    "        #print(f' / Penalty Model / {mdp_model_p.policy}')\n",
    "\n",
    "        R_r_w = - R_r\n",
    "        mdp_model_r_w = mdptoolbox.mdp.PolicyIteration(P_t, R_r_w, 0.99)\n",
    "        mdp_model_r_w.run()\n",
    "\n",
    "        R_p_w = - R_p\n",
    "        mdp_model_p_w = mdptoolbox.mdp.PolicyIteration(P_t, R_p_w, 0.99)\n",
    "        mdp_model_p_w.run()\n",
    "\n",
    "        #policy = {\"r\":mdp_model_r.policy, \"p\":mdp_model_p.policy, \"r_w\" : mdp_model_r_w.policy, \"p_w\" : mdp_model_r_w.policy}\n",
    "        policy = {\"r\":mdp_model_r.policy, \"p\":mdp_model_p.policy, \"r_w\" : mdp_model_r_w.policy, \"p_w\" : mdp_model_r_w.policy}\n",
    "        sim_reward, random_reward, const_reward, sim_reward_w, real_reward = [0,0], [0,0], [0,0], [0,0], [0,0]\n",
    "\n",
    "        tot_n = 0\n",
    "        for pat in valid_idx:\n",
    "            pat_d = valid[valid['PATNO'] == pat]\n",
    "            n = len(pat_d)\n",
    "            init_LEDD = list(pat_d['LEDD'])[0]\n",
    "            LEDD = {\"optimal\":[init_LEDD,init_LEDD],\n",
    "                    \"random\":[init_LEDD,init_LEDD],\n",
    "                    \"const\":[init_LEDD,init_LEDD],\n",
    "                    \"worst\":[init_LEDD,init_LEDD],\n",
    "                    \"real\":[0,0]} #sim_r, sim_p, worst_r, worst_p, constant, random\n",
    "            LEDD_traj = {\"optimal\":[init_LEDD,init_LEDD],\n",
    "                    \"random\":[init_LEDD,init_LEDD],\n",
    "                    \"const\":[init_LEDD,init_LEDD],\n",
    "                    \"worst\":[init_LEDD,init_LEDD],\n",
    "                    \"real\":[0,0]}\n",
    "\n",
    "            state = pat_d['cluster']\n",
    "            state = [list(state)[0],list(state)[0]]\n",
    "            \n",
    "            LEDD_total[\"real\"].append(pat_d[\"LEDD\"].mean())\n",
    "\n",
    "            #simul\n",
    "            for t in tests:\n",
    "                if state[0] == 0:\n",
    "                    if t == \"optimal\":\n",
    "                        sim_reward[0] += 1\n",
    "                    elif t == \"const\":\n",
    "                        const_reward[0] += 1\n",
    "                    elif t == \"worst\":\n",
    "                        sim_reward_w[0] += 1\n",
    "                    elif t == \"real\":\n",
    "                        real_reward[0] += 1\n",
    "                    else:\n",
    "                        random_reward[0] += 1\n",
    "                elif state[1] == 3:\n",
    "                    if t == \"optimal\":\n",
    "                        sim_reward[1] -= 1\n",
    "                    elif t == \"const\":\n",
    "                        const_reward[1] -= 1\n",
    "                    elif t == \"worst\":\n",
    "                        sim_reward_w[1] -= 1\n",
    "                    elif t == \"real\":\n",
    "                        real_reward[1] -= 1\n",
    "                    else:\n",
    "                        random_reward[1] -= 1\n",
    "\n",
    "                for nn in range(n-1): \n",
    "                \n",
    "                    if t == \"optimal\":\n",
    "                        action = [policy['r'][state[0]], policy['p'][state[1]]]\n",
    "                    elif t == \"const\":\n",
    "                        action = [0,0]\n",
    "                    elif t == \"worst\":\n",
    "                        action = [policy[\"r_w\"][state[1]], policy[\"p_w\"][state[1]]]\n",
    "                    elif t == \"real\":\n",
    "                        action = [list(pat_d[\"action\"])[nn],list(pat_d[\"action\"])[nn]]\n",
    "                    else:\n",
    "                        action = [random.choice(range(3)), random.choice(range(3))]    \n",
    "                    if action[0] == 1:\n",
    "                        LEDD[t][0] += 35\n",
    "                    elif action[0] == 2:\n",
    "                        LEDD[t][0] += 180\n",
    "                    else:\n",
    "                        if t != \"const\":\n",
    "                            LEDD[t][0] -= 28\n",
    "\n",
    "                    if action[1] == 1:\n",
    "                        LEDD[t][1] += 35\n",
    "                    elif action[1] == 2:\n",
    "                        LEDD[t][1] += 180\n",
    "                    else:\n",
    "                        if t != \"const\":\n",
    "                            LEDD[t][1] -= 28\n",
    "                    \n",
    "                    \n",
    "                    LEDD_traj[t][0] += LEDD[t][0]\n",
    "                    LEDD_traj[t][1] += LEDD[t][1]\n",
    "\n",
    "                    state_ = [random.choices(range(4), weights = P_v[action[0],state[0],:])[0], random.choices(range(4), weights = P_v[action[1],state[1],:])[0]]\n",
    "\n",
    "                    for k in range(2):\n",
    "                        if action[k] == 1:\n",
    "                            if t == \"optimal\":\n",
    "                                sim_reward[k] -= 0.01\n",
    "                            elif t == \"const\":\n",
    "                                const_reward[k] -= 0.01\n",
    "                            elif t == \"worst\":\n",
    "                                sim_reward_w[k] -= 0.01\n",
    "                            elif t == \"real\":\n",
    "                                real_reward[k] -= 0.025\n",
    "                            else:\n",
    "                                random_reward[k] -= 0.01\n",
    "                        elif action[k] == 2:\n",
    "                            if t == \"optimal\":\n",
    "                                sim_reward[k] -= 0.025\n",
    "                            elif t == \"const\":\n",
    "                                const_reward[k] -= 0.025\n",
    "                            elif t == \"worst\":\n",
    "                                sim_reward_w[k] -= 0.025\n",
    "                            elif t == \"real\":\n",
    "                                real_reward[k] -= 0.025\n",
    "                            else:\n",
    "                                random_reward[k] -= 0.025\n",
    "                    if state[0] == 0:\n",
    "                        if t == \"optimal\":\n",
    "                            sim_reward[0] += 1\n",
    "                        elif t == \"const\":\n",
    "                            const_reward[0] += 1\n",
    "                        elif t == \"worst\":\n",
    "                            sim_reward_w[0] += 1\n",
    "                        elif t == \"real\":\n",
    "                            real_reward[0] += 0.025\n",
    "                        else:\n",
    "                            random_reward[0] += 1\n",
    "                    elif state[1] == 3:\n",
    "                        if t == \"optimal\":\n",
    "                            sim_reward[1] -= 1\n",
    "                        elif t == \"const\":\n",
    "                            const_reward[1] -= 1\n",
    "                        elif t == \"worst\":\n",
    "                            sim_reward_w[1] -= 1\n",
    "                        elif t == \"real\":\n",
    "                            real_reward[1] -= 1\n",
    "                        else:\n",
    "                            random_reward[1] -= 1\n",
    "                    state = state_\n",
    "                \n",
    "                LEDD_traj[t][0] /= n\n",
    "                LEDD_traj[t][1] /= n\n",
    "\n",
    "                LEDD_total[t][0].append(LEDD_traj[t][0])\n",
    "                LEDD_total[t][1].append(LEDD_traj[t][1])\n",
    "\n",
    "            tot_n += n\n",
    "\n",
    "\n",
    "        sim_reward = [s/tot_n for s in sim_reward]\n",
    "        sim_reward_w = [s/tot_n for s in sim_reward_w]\n",
    "        const_reward = [s/tot_n for s in const_reward]\n",
    "        random_reward = [s/tot_n for s in random_reward]\n",
    "        real_rewards = [s/tot_n for s in real_reward]\n",
    "\n",
    "        for kk in range(2):\n",
    "            tot_sim[kk].append(sim_reward[kk])\n",
    "            tot_const[kk].append(const_reward[kk])\n",
    "            tot_random[kk].append(random_reward[kk])\n",
    "            tot_sim_w[kk].append(sim_reward_w[kk])\n",
    "            tot_real[kk].append(real_rewards[kk])\n",
    "\n",
    "    tot1, tot2 = [], []\n",
    "    #Real total\n",
    "\n",
    "#LEDD_total[\"real\"] = np.mean(LEDD_total[\"real\"])\n",
    "\n",
    "for t in tests:\n",
    "    LEDD_total[t][0] = np.mean(LEDD_total[t][0])\n",
    "    LEDD_total[t][1] = np.mean(LEDD_total[t][1])\n",
    "\n",
    "modes = [\"reward\", \"penalty\"]\n",
    "for aa in range(2):\n",
    "    print(modes[aa])\n",
    "    print(\"opti\",  \"\\t\", np.round(np.mean(tot_sim[aa]),4), \"\\t\", np.round(np.std(tot_sim[aa])/sqrt(CV),4))\n",
    "    print(\"worst\", \"\\t\", np.round(np.mean(tot_sim_w[aa]),4), \"\\t\", np.round(np.std(tot_sim_w[aa])/sqrt(CV),4))\n",
    "    print(\"const\",  \"\\t\", np.round(np.mean(tot_const[aa]),4),  \"\\t\", np.round(np.std(tot_const[aa])/sqrt(CV),4))\n",
    "    print(\"random\",  \"\\t\", np.round(np.mean(tot_random[aa]),4), \"\\t\", np.round(np.std(tot_random[aa])/sqrt(CV),4))\n",
    "    print(\"real \\t\", np.round(np.mean(tot_real[aa]),4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ts_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (\u001b[43mts_df\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(ts_df)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ts_df' is not defined"
     ]
    }
   ],
   "source": [
    "(ts_df[\"cluster\"] == 0).sum()/len(ts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
