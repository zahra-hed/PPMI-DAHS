{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 6. MDP model solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox, mdptoolbox.example\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read probability matrix from part 5\n",
    "ts_df = pd.read_csv(\"./dataset/generated_data/ts_df.csv\")\n",
    "ts_df = ts_df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = ts_df.rename(columns={'action': 'action_0', 'action_2': 'action'})\n",
    "# ts_df.fillna(-1, inplace = True)\n",
    "# ts_df['action'] = ts_df['action'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_CV(input, CV):\n",
    "    data_list = list(input)\n",
    "    random.shuffle(data_list)\n",
    "\n",
    "    subset_size = len(data_list) // CV\n",
    "    remainder = len(data_list) % CV\n",
    "\n",
    "    subsets = []\n",
    "    start_idx = 0\n",
    "    for i in range(CV):\n",
    "        end_idx = start_idx + subset_size + (1 if i < remainder else 0)\n",
    "        subsets.append(data_list[start_idx:end_idx])\n",
    "        start_idx = end_idx\n",
    "    return subsets\n",
    "\n",
    "#define the reward function\n",
    "def Reward(a, b, mode):\n",
    "    R = np.zeros(12).reshape(4,3)\n",
    "    if mode == \"reward\":\n",
    "        R[0,:] += 1\n",
    "    else:\n",
    "        R[3,:] -= 1\n",
    "    R[:,1] -= a\n",
    "    R[:,2] -= b\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REWARD\n",
      "opti \t 0.3505 \t 0.0114\n",
      "worst \t 0.2891 \t 0.0082\n",
      "const \t 0.313 \t 0.0083\n",
      "random \t 0.2834 \t 0.0066\n",
      "VI \t 0.3349 \t 0.0059\n",
      "VI w \t 0.3004 \t 0.0095\n",
      "real \t 0.3084\n",
      "\n",
      "PENALTY\n",
      "opti \t -0.1275 \t 0.0076\n",
      "worst \t -0.1444 \t 0.0064\n",
      "const \t -0.1261 \t 0.0099\n",
      "random \t -0.3005 \t 0.0069\n",
      "VI \t -0.1143 \t 0.0058\n",
      "VI w \t 0.0 \t 0.0\n",
      "real \t -0.1307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# divide the data into 1:9\n",
    "# calculate P\n",
    "# solve the MDP (for 2)\n",
    "# \n",
    "CV = 10\n",
    "\n",
    "patients = ts_df.iloc[:,0]\n",
    "patients = set(patients)\n",
    "subsets = divide_CV(patients, CV)\n",
    "\n",
    "tot_sim, tot_const, tot_random, tot_sim_w, tot_real, tot_vi, tot_vi_w = [[],[]], [[],[]], [[],[]], [[],[]], [[],[]], [[],[]], [[],[]]\n",
    "LEDD_total = [0 for _ in range(8)] #sim_r, sim_w, worst_p, worst_p, constant, random, actual \n",
    "LEDD_total = {\"optimal\":[[],[]],\n",
    "                \"random\":[[],[]],\n",
    "                \"const\":[[],[]],\n",
    "                \"worst\":[[],[]],\n",
    "                \"real\":[[],[]],\n",
    "                \"vi\": [[],[]],\n",
    "                \"vi_worst\": [[],[]]}\n",
    "tests = [\"optimal\", \"random\", \"const\", \"worst\", \"real\", \"vi\", \"vi_worst\"]\n",
    "\n",
    "for i in range(CV+1):\n",
    "    if i < CV:\n",
    "        valid_idx = subsets[i]\n",
    "        train_idx = []\n",
    "        for j in range(CV):\n",
    "            if i != j:\n",
    "                train_idx += subsets[j]\n",
    "\n",
    "        valid = ts_df[ts_df['PATNO'].isin(valid_idx)]\n",
    "        train = ts_df[ts_df['PATNO'].isin(train_idx)]\n",
    "    else:\n",
    "        valid = ts_df\n",
    "\n",
    "    #create probablity matrix P\n",
    "    P_t = np.zeros(4*3*4).reshape(3,4,4)\n",
    "    P_v = np.zeros(4*3*4).reshape(3,4,4)\n",
    "\n",
    "    for a in range(3):\n",
    "        for s in range(4):\n",
    "            for s_ in range(3):\n",
    "                criteria = ((train['cluster'] == s) & (train['action'] == a))\n",
    "                criteria_2 = ((train['cluster'] == s) & (train['action'] == a) & (train['cluster_n'] == s_))\n",
    "                P_t[a,s,s_] = criteria_2.sum()/criteria.sum()\n",
    "\n",
    "            P_t[a,s,3] = 1- sum(P_t[a,s,:])\n",
    "\n",
    "    for a in range(3):\n",
    "        for s in range(4):\n",
    "            for s_ in range(3):\n",
    "                criteria = ((ts_df['cluster'] == s) & (ts_df['action'] == a))\n",
    "                criteria_2 = ((ts_df['cluster'] == s) & (ts_df['action'] == a) & (ts_df['cluster_n'] == s_))\n",
    "                P_v[a,s,s_] = criteria_2.sum()/criteria.sum()\n",
    "\n",
    "            P_v[a,s,3] = 1 - sum(P_v[a,s,:])\n",
    "\n",
    "\n",
    "    a, b = 0.01, 0.025\n",
    "    R_r = Reward(a, b,\"reward\")\n",
    "\n",
    "    \n",
    "    mdp_model_r = mdptoolbox.mdp.PolicyIteration(P_t, R_r, 0.99)\n",
    "    mdp_model_r.run()\n",
    "    # if i < CV:\n",
    "    #     print(f'CV = {i+1} / Reward Model / {mdp_model_r.policy}' , end = \"\")\n",
    "    # else:\n",
    "    #     print(f'Full D / Reward Model / {mdp_model_r.policy}' , end = \"\")\n",
    "\n",
    "    R_p = Reward(a, b,\"penalty\")\n",
    "    mdp_model_p = mdptoolbox.mdp.PolicyIteration(P_t, R_p, 0.99)\n",
    "    mdp_model_p.run()\n",
    "    #print(f' / Penalty Model / {mdp_model_p.policy}')\n",
    "\n",
    "    R_r_w = - R_r\n",
    "    mdp_model_r_w = mdptoolbox.mdp.PolicyIteration(P_t, R_r_w, 0.99)\n",
    "    mdp_model_r_w.run()\n",
    "\n",
    "    R_p_w = - R_p\n",
    "    mdp_model_p_w = mdptoolbox.mdp.PolicyIteration(P_t, R_p_w, 0.99)\n",
    "    mdp_model_p_w.run()\n",
    "\n",
    "    \n",
    "    # Value iteration models\n",
    "    vi_model_r = mdptoolbox.mdp.ValueIteration(P_t, R_r, 0.99)\n",
    "    vi_model_r.run()\n",
    "\n",
    "    vi_model_p = mdptoolbox.mdp.ValueIteration(P_t, R_p, 0.99)\n",
    "    vi_model_p.run()\n",
    "    \n",
    "    # Value iteration models\n",
    "    vi_model_r_w = mdptoolbox.mdp.ValueIteration(P_t, R_r_w, 0.99)\n",
    "    vi_model_r_w.run()\n",
    "\n",
    "    vi_model_p_w = mdptoolbox.mdp.ValueIteration(P_t, R_p_w, 0.99)\n",
    "    vi_model_p_w.run()\n",
    "\n",
    "    #policy = {\"r\":mdp_model_r.policy, \"p\":mdp_model_p.policy, \"r_w\" : mdp_model_r_w.policy, \"p_w\" : mdp_model_r_w.policy}\n",
    "    policy = {\n",
    "        \"r\":mdp_model_r.policy,\n",
    "        \"p\":mdp_model_p.policy,\n",
    "        \"r_w\" : mdp_model_r_w.policy,\n",
    "        \"p_w\" : mdp_model_r_w.policy,\n",
    "        \"vi_r\": vi_model_r.policy,\n",
    "        \"vi_p\": vi_model_p.policy,\n",
    "        \"vi_r_w\": vi_model_r_w.policy,\n",
    "        \"vi_p_w\": vi_model_p_w.policy\n",
    "        }\n",
    "    \n",
    "    sim_reward, random_reward, const_reward, sim_reward_w, real_reward, vi_reward, vi_reward_w = [0,0], [0,0], [0,0], [0,0], [0,0], [0,0], [0,0]\n",
    "\n",
    "    tot_n = 0\n",
    "    for pat in valid_idx:\n",
    "        pat_d = valid[valid['PATNO'] == pat]\n",
    "        n = len(pat_d)\n",
    "        init_LEDD = list(pat_d['LEDD'])[0]\n",
    "        LEDD = {\"optimal\":[init_LEDD,init_LEDD],\n",
    "                \"random\":[init_LEDD,init_LEDD],\n",
    "                \"const\":[init_LEDD,init_LEDD],\n",
    "                \"worst\":[init_LEDD,init_LEDD],\n",
    "                \"real\":[0,0],\n",
    "                \"vi\": [init_LEDD, init_LEDD],\n",
    "                \"vi_worst\": [init_LEDD, init_LEDD]} #sim_r, sim_p, worst_r, worst_p, constant, random\n",
    "        \n",
    "        LEDD_traj = {\"optimal\":[init_LEDD,init_LEDD],\n",
    "                \"random\":[init_LEDD,init_LEDD],\n",
    "                \"const\":[init_LEDD,init_LEDD],\n",
    "                \"worst\":[init_LEDD,init_LEDD],\n",
    "                \"real\":[0,0],\n",
    "                \"vi\":[init_LEDD, init_LEDD],\n",
    "                \"vi_worst\":[init_LEDD, init_LEDD]}\n",
    "\n",
    "        state = pat_d['cluster']\n",
    "        state = [list(state)[0],list(state)[0]]\n",
    "        \n",
    "        LEDD_total[\"real\"].append(pat_d[\"LEDD\"].mean())\n",
    "        \n",
    "        #simul\n",
    "        for t in tests:\n",
    "            if state[0] == 0:\n",
    "                if t == \"optimal\":\n",
    "                    sim_reward[0] += 1\n",
    "                elif t == \"const\":\n",
    "                    const_reward[0] += 1\n",
    "                elif t == \"worst\":\n",
    "                    sim_reward_w[0] += 1\n",
    "                elif t == \"real\":\n",
    "                    real_reward[0] += 1\n",
    "                elif t == \"vi\":\n",
    "                    vi_reward[0] += 1\n",
    "                elif t == \"vi_worst\":\n",
    "                    vi_reward_w[0] += 1\n",
    "                else:\n",
    "                    random_reward[0] += 1\n",
    "            elif state[1] == 3:\n",
    "                if t == \"optimal\":\n",
    "                    sim_reward[1] -= 1\n",
    "                elif t == \"const\":\n",
    "                    const_reward[1] -= 1\n",
    "                elif t == \"worst\":\n",
    "                    sim_reward_w[1] -= 1\n",
    "                elif t == \"real\":\n",
    "                    real_reward[1] -= 1\n",
    "                elif t == \"vi\":\n",
    "                    vi_reward[1] -= 1\n",
    "                elif t == \"vi_reward_w\":\n",
    "                    vi_reward_w[1] -= 1\n",
    "                else:\n",
    "                    random_reward[1] -= 1\n",
    "\n",
    "            for nn in range(n-1): \n",
    "            \n",
    "                if t == \"optimal\":\n",
    "                    action = [policy['r'][state[0]], policy['p'][state[1]]]\n",
    "                elif t == \"const\":\n",
    "                    action = [0,0]\n",
    "                elif t == \"worst\":\n",
    "                    action = [policy[\"r_w\"][state[1]], policy[\"p_w\"][state[1]]]\n",
    "                elif t == \"real\":\n",
    "                    action = [list(pat_d[\"action\"])[nn],list(pat_d[\"action\"])[nn]]\n",
    "                elif t == \"vi\":\n",
    "                    action = [policy['vi_r'][state[0]], policy['vi_p'][state[1]]]\n",
    "                elif t == \"vi_worst\":\n",
    "                    action = [policy['vi_r_w'][state[0]], policy['vi_p_w'][state[1]]]\n",
    "                else:\n",
    "                    action = [random.choice(range(3)), random.choice(range(3))]    \n",
    "\n",
    "                if action[0] == 1:\n",
    "                    LEDD[t][0] += 35\n",
    "                elif action[0] == 2:\n",
    "                    LEDD[t][0] += 180\n",
    "                else:\n",
    "                    if t != \"const\":\n",
    "                        LEDD[t][0] -= 28\n",
    "\n",
    "                if action[1] == 1:\n",
    "                    LEDD[t][1] += 35\n",
    "                elif action[1] == 2:\n",
    "                    LEDD[t][1] += 180\n",
    "                else:\n",
    "                    if t != \"const\":\n",
    "                        LEDD[t][1] -= 28\n",
    "                \n",
    "                \n",
    "                LEDD_traj[t][0] += LEDD[t][0]\n",
    "                LEDD_traj[t][1] += LEDD[t][1]\n",
    "\n",
    "                state_ = [\n",
    "                    random.choices(range(4), weights = P_v[int(action[0]),int(state[0]),:])[0],\n",
    "                    random.choices(range(4), weights = P_v[int(action[1]),int(state[1]),:])[0]\n",
    "                ]\n",
    "\n",
    "                for k in range(2):\n",
    "                    if action[k] == 1:\n",
    "                        if t == \"optimal\":\n",
    "                            sim_reward[k] -= 0.01\n",
    "                        elif t == \"const\":\n",
    "                            const_reward[k] -= 0.01\n",
    "                        elif t == \"worst\":\n",
    "                            sim_reward_w[k] -= 0.01\n",
    "                        elif t == \"real\":\n",
    "                            real_reward[k] -= 0.01\n",
    "                        elif t == \"vi_reward\":\n",
    "                            vi_reward[k] -= 0.01\n",
    "                        elif t == \"vi_reward_w\":\n",
    "                            vi_reward_w[k] -= 0.01\n",
    "                        else:\n",
    "                            random_reward[k] -= 0.01\n",
    "                    elif action[k] == 2:\n",
    "                        if t == \"optimal\":\n",
    "                            sim_reward[k] -= 0.025\n",
    "                        elif t == \"const\":\n",
    "                            const_reward[k] -= 0.025\n",
    "                        elif t == \"worst\":\n",
    "                            sim_reward_w[k] -= 0.025\n",
    "                        elif t == \"real\":\n",
    "                            real_reward[k] -= 0.025\n",
    "                        elif t == \"vi_reward\":\n",
    "                            vi_reward[k] -= 0.025\n",
    "                        elif t == \"vi_reward_w\":\n",
    "                            vi_reward_w[k] -= 0.025\n",
    "                        else:\n",
    "                            random_reward[k] -= 0.025\n",
    "                if state[0] == 0:\n",
    "                    if t == \"optimal\":\n",
    "                        sim_reward[0] += 1\n",
    "                    elif t == \"const\":\n",
    "                        const_reward[0] += 1\n",
    "                    elif t == \"worst\":\n",
    "                        sim_reward_w[0] += 1\n",
    "                    elif t == \"real\":\n",
    "                        real_reward[0] += 1\n",
    "                    elif t == \"vi\":\n",
    "                        vi_reward[0] += 1\n",
    "                    elif t == \"vi_worst\":\n",
    "                        vi_reward_w[0] += 1\n",
    "                    else:\n",
    "                        random_reward[0] += 1\n",
    "                elif state[1] == 3:\n",
    "                    if t == \"optimal\":\n",
    "                        sim_reward[1] -= 1\n",
    "                    elif t == \"const\":\n",
    "                        const_reward[1] -= 1\n",
    "                    elif t == \"worst\":\n",
    "                        sim_reward_w[1] -= 1\n",
    "                    elif t == \"real\":\n",
    "                        real_reward[1] -= 1\n",
    "                    elif t == \"vi\":\n",
    "                        vi_reward[1] -= 1\n",
    "                    elif t == \"vi_reward_w\":\n",
    "                        vi_reward_w[1] -= 1\n",
    "                    else:\n",
    "                        random_reward[1] -= 1\n",
    "                state = state_\n",
    "            \n",
    "            LEDD_traj[t][0] /= n\n",
    "            LEDD_traj[t][1] /= n\n",
    "\n",
    "            LEDD_total[t][0].append(LEDD_traj[t][0])\n",
    "            LEDD_total[t][1].append(LEDD_traj[t][1])\n",
    "\n",
    "        tot_n += n\n",
    "\n",
    "\n",
    "    sim_reward = [s/tot_n for s in sim_reward]\n",
    "    sim_reward_w = [s/tot_n for s in sim_reward_w]\n",
    "    const_reward = [s/tot_n for s in const_reward]\n",
    "    random_reward = [s/tot_n for s in random_reward]\n",
    "    real_reward = [s/tot_n for s in real_reward]\n",
    "    vi_reward = [s/tot_n for s in vi_reward]\n",
    "    vi_reward_w = [s/tot_n for s in vi_reward_w]\n",
    "\n",
    "\n",
    "    for kk in range(2):\n",
    "        tot_sim[kk].append(sim_reward[kk])\n",
    "        tot_const[kk].append(const_reward[kk])\n",
    "        tot_random[kk].append(random_reward[kk])\n",
    "        tot_sim_w[kk].append(sim_reward_w[kk])\n",
    "        tot_real[kk].append(real_reward[kk])\n",
    "        tot_vi[kk].append(vi_reward[kk])\n",
    "        tot_vi_w[kk].append(vi_reward_w[kk])\n",
    "\n",
    "tot1, tot2 = [], []\n",
    "#Real total\n",
    "\n",
    "#LEDD_total[\"real\"] = np.mean(LEDD_total[\"real\"])\n",
    "\n",
    "for t in tests:\n",
    "    LEDD_total[t][0] = np.mean(LEDD_total[t][0])\n",
    "    LEDD_total[t][1] = np.mean(LEDD_total[t][1])\n",
    "    \n",
    "\n",
    "modes = [\"reward\", \"penalty\"]\n",
    "for aa in range(2):\n",
    "    print(modes[aa].upper())\n",
    "    print(\"opti\",  \"\\t\", np.round(np.mean(tot_sim[aa]),4), \"\\t\", np.round(np.std(tot_sim[aa])/sqrt(CV),4))\n",
    "    print(\"worst\", \"\\t\", np.round(np.mean(tot_sim_w[aa]),4), \"\\t\", np.round(np.std(tot_sim_w[aa])/sqrt(CV),4))\n",
    "    print(\"const\",  \"\\t\", np.round(np.mean(tot_const[aa]),4),  \"\\t\", np.round(np.std(tot_const[aa])/sqrt(CV),4))\n",
    "    print(\"random\",  \"\\t\", np.round(np.mean(tot_random[aa]),4), \"\\t\", np.round(np.std(tot_random[aa])/sqrt(CV),4))\n",
    "    print(\"VI\",  \"\\t\", np.round(np.mean(tot_vi[aa]),4), \"\\t\", np.round(np.std(tot_vi[aa])/sqrt(CV),4))\n",
    "    print(\"VI w\",  \"\\t\", np.round(np.mean(tot_vi_w[aa]),4), \"\\t\", np.round(np.std(tot_vi_w[aa])/sqrt(CV),4))\n",
    "    print(\"real \\t\", np.round(np.mean(tot_real[aa]),4))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 161\u001b[0m\n\u001b[0;32m    158\u001b[0m LEDD_traj[t][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m LEDD[t][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    159\u001b[0m LEDD_traj[t][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m LEDD[t][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 161\u001b[0m state_ \u001b[38;5;241m=\u001b[39m [random\u001b[38;5;241m.\u001b[39mchoices(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m), weights \u001b[38;5;241m=\u001b[39m \u001b[43mP_v\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)[\u001b[38;5;241m0\u001b[39m], random\u001b[38;5;241m.\u001b[39mchoices(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m), weights \u001b[38;5;241m=\u001b[39m P_v[action[\u001b[38;5;241m1\u001b[39m],state[\u001b[38;5;241m1\u001b[39m],:])[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action[k] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# divide the data into 1:9\n",
    "# calculate P\n",
    "# solve the MDP (for 2)\n",
    "# \n",
    "CV = 10\n",
    "\n",
    "patients = ts_df.iloc[:,0]\n",
    "patients = set(patients)\n",
    "subsets = divide_CV(patients, CV)\n",
    "\n",
    "tot_sim, tot_const, tot_random, tot_sim_w, tot_real = [[],[]], [[],[]], [[],[]], [[],[]], [[],[]]\n",
    "LEDD_total = [0 for _ in range(7)] #sim_r, sim_w, worst_p, worst_p, constant, random, actual \n",
    "LEDD_total = {\"optimal\":[[],[]],\n",
    "                \"random\":[[],[]],\n",
    "                \"const\":[[],[]],\n",
    "                \"worst\":[[],[]],\n",
    "                \"real\":[[],[]]}\n",
    "tests = [\"optimal\", \"random\", \"const\", \"worst\", \"real\"]\n",
    "\n",
    "for i in range(CV+1):\n",
    "    for _ in range(1):\n",
    "        if i < CV:\n",
    "            valid_idx = subsets[i]\n",
    "            train_idx = []\n",
    "            for j in range(CV):\n",
    "                if i != j:\n",
    "                    train_idx += subsets[j]\n",
    "\n",
    "            valid = ts_df[ts_df['PATNO'].isin(valid_idx)]\n",
    "            train = ts_df[ts_df['PATNO'].isin(train_idx)]\n",
    "        else:\n",
    "            valid = ts_df\n",
    "\n",
    "        #create probablity matrix P\n",
    "        P_t = np.zeros(4*3*4).reshape(3,4,4)\n",
    "        P_v = np.zeros(4*3*4).reshape(3,4,4)\n",
    "\n",
    "        for a in range(3):\n",
    "            for s in range(4):\n",
    "                for s_ in range(3):\n",
    "                    criteria = ((train['cluster'] == s) & (train['action'] == a))\n",
    "                    criteria_2 = ((train['cluster'] == s) & (train['action'] == a) & (train['cluster_n'] == s_))\n",
    "                    P_t[a,s,s_] = criteria_2.sum()/criteria.sum()\n",
    "\n",
    "                P_t[a,s,3] = 1- sum(P_t[a,s,:])\n",
    "\n",
    "        for a in range(3):\n",
    "            for s in range(4):\n",
    "                for s_ in range(3):\n",
    "                    criteria = ((ts_df['cluster'] == s) & (ts_df['action'] == a))\n",
    "                    criteria_2 = ((ts_df['cluster'] == s) & (ts_df['action'] == a) & (ts_df['cluster_n'] == s_))\n",
    "                    P_v[a,s,s_] = criteria_2.sum()/criteria.sum()\n",
    "\n",
    "                P_v[a,s,3] = 1 - sum(P_v[a,s,:])\n",
    "\n",
    "\n",
    "        a, b = 0.01, 0.025\n",
    "        R_r = Reward(a, b,\"reward\")\n",
    "        mdp_model_r = mdptoolbox.mdp.PolicyIteration(P_t, R_r, 0.99)\n",
    "        mdp_model_r.run()\n",
    "        # if i < CV:\n",
    "        #     print(f'CV = {i+1} / Reward Model / {mdp_model_r.policy}' , end = \"\")\n",
    "        # else:\n",
    "        #     print(f'Full D / Reward Model / {mdp_model_r.policy}' , end = \"\")\n",
    "\n",
    "        R_p = Reward(a, b,\"penalty\")\n",
    "        mdp_model_p = mdptoolbox.mdp.PolicyIteration(P_t, R_p, 0.99)\n",
    "        mdp_model_p.run()\n",
    "        #print(f' / Penalty Model / {mdp_model_p.policy}')\n",
    "\n",
    "        R_r_w = - R_r\n",
    "        mdp_model_r_w = mdptoolbox.mdp.PolicyIteration(P_t, R_r_w, 0.99)\n",
    "        mdp_model_r_w.run()\n",
    "\n",
    "        R_p_w = - R_p\n",
    "        mdp_model_p_w = mdptoolbox.mdp.PolicyIteration(P_t, R_p_w, 0.99)\n",
    "        mdp_model_p_w.run()\n",
    "\n",
    "        #policy = {\"r\":mdp_model_r.policy, \"p\":mdp_model_p.policy, \"r_w\" : mdp_model_r_w.policy, \"p_w\" : mdp_model_r_w.policy}\n",
    "        policy = {\"r\":mdp_model_r.policy, \"p\":mdp_model_p.policy, \"r_w\" : mdp_model_r_w.policy, \"p_w\" : mdp_model_r_w.policy}\n",
    "        sim_reward, random_reward, const_reward, sim_reward_w, real_reward = [0,0], [0,0], [0,0], [0,0], [0,0]\n",
    "\n",
    "        tot_n = 0\n",
    "        for pat in valid_idx:\n",
    "            pat_d = valid[valid['PATNO'] == pat]\n",
    "            n = len(pat_d)\n",
    "            init_LEDD = list(pat_d['LEDD'])[0]\n",
    "            LEDD = {\"optimal\":[init_LEDD,init_LEDD],\n",
    "                    \"random\":[init_LEDD,init_LEDD],\n",
    "                    \"const\":[init_LEDD,init_LEDD],\n",
    "                    \"worst\":[init_LEDD,init_LEDD],\n",
    "                    \"real\":[0,0]} #sim_r, sim_p, worst_r, worst_p, constant, random\n",
    "            LEDD_traj = {\"optimal\":[init_LEDD,init_LEDD],\n",
    "                    \"random\":[init_LEDD,init_LEDD],\n",
    "                    \"const\":[init_LEDD,init_LEDD],\n",
    "                    \"worst\":[init_LEDD,init_LEDD],\n",
    "                    \"real\":[0,0]}\n",
    "\n",
    "            state = pat_d['cluster']\n",
    "            state = [list(state)[0],list(state)[0]]\n",
    "            \n",
    "            LEDD_total[\"real\"].append(pat_d[\"LEDD\"].mean())\n",
    "\n",
    "            #simul\n",
    "            for t in tests:\n",
    "                if state[0] == 0:\n",
    "                    if t == \"optimal\":\n",
    "                        sim_reward[0] += 1\n",
    "                    elif t == \"const\":\n",
    "                        const_reward[0] += 1\n",
    "                    elif t == \"worst\":\n",
    "                        sim_reward_w[0] += 1\n",
    "                    elif t == \"real\":\n",
    "                        real_reward[0] += 1\n",
    "                    else:\n",
    "                        random_reward[0] += 1\n",
    "                elif state[1] == 3:\n",
    "                    if t == \"optimal\":\n",
    "                        sim_reward[1] -= 1\n",
    "                    elif t == \"const\":\n",
    "                        const_reward[1] -= 1\n",
    "                    elif t == \"worst\":\n",
    "                        sim_reward_w[1] -= 1\n",
    "                    elif t == \"real\":\n",
    "                        real_reward[1] -= 1\n",
    "                    else:\n",
    "                        random_reward[1] -= 1\n",
    "\n",
    "                for nn in range(n-1): \n",
    "                \n",
    "                    if t == \"optimal\":\n",
    "                        action = [policy['r'][state[0]], policy['p'][state[1]]]\n",
    "                    elif t == \"const\":\n",
    "                        action = [0,0]\n",
    "                    elif t == \"worst\":\n",
    "                        action = [policy[\"r_w\"][state[1]], policy[\"p_w\"][state[1]]]\n",
    "                    elif t == \"real\":\n",
    "                        action = [list(pat_d[\"action\"])[nn],list(pat_d[\"action\"])[nn]]\n",
    "                    else:\n",
    "                        action = [random.choice(range(3)), random.choice(range(3))]    \n",
    "                    if action[0] == 1:\n",
    "                        LEDD[t][0] += 35\n",
    "                    elif action[0] == 2:\n",
    "                        LEDD[t][0] += 180\n",
    "                    else:\n",
    "                        if t != \"const\":\n",
    "                            LEDD[t][0] -= 28\n",
    "\n",
    "                    if action[1] == 1:\n",
    "                        LEDD[t][1] += 35\n",
    "                    elif action[1] == 2:\n",
    "                        LEDD[t][1] += 180\n",
    "                    else:\n",
    "                        if t != \"const\":\n",
    "                            LEDD[t][1] -= 28\n",
    "                    \n",
    "                    \n",
    "                    LEDD_traj[t][0] += LEDD[t][0]\n",
    "                    LEDD_traj[t][1] += LEDD[t][1]\n",
    "\n",
    "                    state_ = [random.choices(range(4), weights = P_v[action[0],state[0],:])[0], random.choices(range(4), weights = P_v[action[1],state[1],:])[0]]\n",
    "\n",
    "                    for k in range(2):\n",
    "                        if action[k] == 1:\n",
    "                            if t == \"optimal\":\n",
    "                                sim_reward[k] -= 0.01\n",
    "                            elif t == \"const\":\n",
    "                                const_reward[k] -= 0.01\n",
    "                            elif t == \"worst\":\n",
    "                                sim_reward_w[k] -= 0.01\n",
    "                            elif t == \"real\":\n",
    "                                real_reward[k] -= 0.025\n",
    "                            else:\n",
    "                                random_reward[k] -= 0.01\n",
    "                        elif action[k] == 2:\n",
    "                            if t == \"optimal\":\n",
    "                                sim_reward[k] -= 0.025\n",
    "                            elif t == \"const\":\n",
    "                                const_reward[k] -= 0.025\n",
    "                            elif t == \"worst\":\n",
    "                                sim_reward_w[k] -= 0.025\n",
    "                            elif t == \"real\":\n",
    "                                real_reward[k] -= 0.025\n",
    "                            else:\n",
    "                                random_reward[k] -= 0.025\n",
    "                    if state[0] == 0:\n",
    "                        if t == \"optimal\":\n",
    "                            sim_reward[0] += 1\n",
    "                        elif t == \"const\":\n",
    "                            const_reward[0] += 1\n",
    "                        elif t == \"worst\":\n",
    "                            sim_reward_w[0] += 1\n",
    "                        elif t == \"real\":\n",
    "                            real_reward[0] += 0.025\n",
    "                        else:\n",
    "                            random_reward[0] += 1\n",
    "                    elif state[1] == 3:\n",
    "                        if t == \"optimal\":\n",
    "                            sim_reward[1] -= 1\n",
    "                        elif t == \"const\":\n",
    "                            const_reward[1] -= 1\n",
    "                        elif t == \"worst\":\n",
    "                            sim_reward_w[1] -= 1\n",
    "                        elif t == \"real\":\n",
    "                            real_reward[1] -= 1\n",
    "                        else:\n",
    "                            random_reward[1] -= 1\n",
    "                    state = state_\n",
    "                \n",
    "                LEDD_traj[t][0] /= n\n",
    "                LEDD_traj[t][1] /= n\n",
    "\n",
    "                LEDD_total[t][0].append(LEDD_traj[t][0])\n",
    "                LEDD_total[t][1].append(LEDD_traj[t][1])\n",
    "\n",
    "            tot_n += n\n",
    "\n",
    "\n",
    "        sim_reward = [s/tot_n for s in sim_reward]\n",
    "        sim_reward_w = [s/tot_n for s in sim_reward_w]\n",
    "        const_reward = [s/tot_n for s in const_reward]\n",
    "        random_reward = [s/tot_n for s in random_reward]\n",
    "        real_rewards = [s/tot_n for s in real_reward]\n",
    "\n",
    "        for kk in range(2):\n",
    "            tot_sim[kk].append(sim_reward[kk])\n",
    "            tot_const[kk].append(const_reward[kk])\n",
    "            tot_random[kk].append(random_reward[kk])\n",
    "            tot_sim_w[kk].append(sim_reward_w[kk])\n",
    "            tot_real[kk].append(real_rewards[kk])\n",
    "\n",
    "    tot1, tot2 = [], []\n",
    "    #Real total\n",
    "\n",
    "#LEDD_total[\"real\"] = np.mean(LEDD_total[\"real\"])\n",
    "\n",
    "for t in tests:\n",
    "    LEDD_total[t][0] = np.mean(LEDD_total[t][0])\n",
    "    LEDD_total[t][1] = np.mean(LEDD_total[t][1])\n",
    "\n",
    "modes = [\"reward\", \"penalty\"]\n",
    "for aa in range(2):\n",
    "    print(modes[aa])\n",
    "    print(\"opti\",  \"\\t\", np.round(np.mean(tot_sim[aa]),4), \"\\t\", np.round(np.std(tot_sim[aa])/sqrt(CV),4))\n",
    "    print(\"worst\", \"\\t\", np.round(np.mean(tot_sim_w[aa]),4), \"\\t\", np.round(np.std(tot_sim_w[aa])/sqrt(CV),4))\n",
    "    print(\"const\",  \"\\t\", np.round(np.mean(tot_const[aa]),4),  \"\\t\", np.round(np.std(tot_const[aa])/sqrt(CV),4))\n",
    "    print(\"random\",  \"\\t\", np.round(np.mean(tot_random[aa]),4), \"\\t\", np.round(np.std(tot_random[aa])/sqrt(CV),4))\n",
    "    print(\"real \\t\", np.round(np.mean(tot_real[aa]),4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2621580547112462"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ts_df[\"cluster\"] == 0).sum()/len(ts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
