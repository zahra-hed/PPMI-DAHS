{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 6. MDP model solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox, mdptoolbox.example\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read probability matrix from part 5\n",
    "ts_df = pd.read_csv(\"./dataset/generated_data/ts_df_9.csv\")\n",
    "ts_df = ts_df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = ts_df.rename(columns={'action': 'action_0', 'action_2': 'action'})\n",
    "# ts_df.fillna(-1, inplace = True)\n",
    "# ts_df['action'] = ts_df['action'].astype(int)\n",
    "num_states = 9\n",
    "num_actions = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_CV(input, CV):\n",
    "    data_list = list(input)\n",
    "    random.shuffle(data_list)\n",
    "\n",
    "    subset_size = len(data_list) // CV\n",
    "    remainder = len(data_list) % CV\n",
    "\n",
    "    subsets = []\n",
    "    start_idx = 0\n",
    "    for i in range(CV):\n",
    "        end_idx = start_idx + subset_size + (1 if i < remainder else 0)\n",
    "        subsets.append(data_list[start_idx:end_idx])\n",
    "        start_idx = end_idx\n",
    "    return subsets\n",
    "\n",
    "#define the reward function\n",
    "def Reward(a, b, mode, num_states, num_actions):\n",
    "    R = np.zeros(num_states * num_actions).reshape(num_states,num_actions)\n",
    "    if mode == \"reward\":\n",
    "        R[0,:] += 1\n",
    "\n",
    "    else:\n",
    "        R[num_states - 1,:] -= 1\n",
    "\n",
    "    \n",
    "    R[:,1] -= a\n",
    "    R[:,2] -= b\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Actions chosen for optimal, reward mode : 0s=3697, 1s=558, 2s=459`\n",
      "`Actions chosen for random, reward mode : 0s=1574, 1s=1576, 2s=1564`\n",
      "`Actions chosen for const, reward mode : 0s=4714, 1s=0, 2s=0`\n",
      "`Actions chosen for worst, reward mode : 0s=0, 1s=672, 2s=4042`\n",
      "`Actions chosen for real, reward mode : 0s=2927, 1s=1329, 2s=458`\n",
      "\n",
      "`Actions chosen for optimal, penalty mode : 0s=2451, 1s=1699, 2s=564`\n",
      "`Actions chosen for random, penalty mode : 0s=1594, 1s=1589, 2s=1531`\n",
      "`Actions chosen for const, penalty mode : 0s=4714, 1s=0, 2s=0`\n",
      "`Actions chosen for worst, penalty mode : 0s=0, 1s=678, 2s=4036`\n",
      "`Actions chosen for real, penalty mode : 0s=2927, 1s=1329, 2s=458`\n",
      "\n",
      "reward\n",
      "opti \t 0.1494 \t 0.0051\n",
      "worst \t 0.0861 \t 0.0051\n",
      "const \t 0.1356 \t 0.004\n",
      "random \t 0.1274 \t 0.0036\n",
      "real \t 0.0049\n",
      "penalty\n",
      "opti \t -0.1435 \t 0.0045\n",
      "worst \t -0.129 \t 0.0052\n",
      "const \t -0.138 \t 0.006\n",
      "random \t -0.1203 \t 0.0038\n",
      "real \t -0.1228\n"
     ]
    }
   ],
   "source": [
    "# divide the data into 1:9\n",
    "# calculate P\n",
    "# solve the MDP (for 2)\n",
    "# \n",
    "CV = 10\n",
    "\n",
    "patients = ts_df.iloc[:,0]\n",
    "patients = set(patients)\n",
    "subsets = divide_CV(patients, CV)\n",
    "\n",
    "tot_sim, tot_const, tot_random, tot_sim_w, tot_real = [[],[]], [[],[]], [[],[]], [[],[]], [[],[]]\n",
    "LEDD_total = [0 for _ in range(7)] #sim_r, sim_w, worst_p, worst_p, constant, random, actual \n",
    "LEDD_total = {\"optimal\":[[],[]],\n",
    "                \"random\":[[],[]],\n",
    "                \"const\":[[],[]],\n",
    "                \"worst\":[[],[]],\n",
    "                \"real\":[[],[]]}\n",
    "tests = [\"optimal\", \"random\", \"const\", \"worst\", \"real\"]\n",
    "actions_chosen = {\n",
    "        t: [[],[]] for t in tests\n",
    "        }\n",
    "for i in range(CV+1):\n",
    "    if i < CV:\n",
    "        valid_idx = subsets[i]\n",
    "        train_idx = []\n",
    "        for j in range(CV):\n",
    "            if i != j:\n",
    "                train_idx += subsets[j]\n",
    "\n",
    "        valid = ts_df[ts_df['PATNO'].isin(valid_idx)]\n",
    "        train = ts_df[ts_df['PATNO'].isin(train_idx)]\n",
    "    else:\n",
    "        valid = ts_df\n",
    "\n",
    "    #create probablity matrix P\n",
    "    P_t = np.zeros(num_states*num_actions*num_states).reshape(num_actions, num_states, num_states)\n",
    "    P_v = np.zeros(num_states*num_actions*num_states).reshape(num_actions, num_states, num_states)\n",
    "\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            for s_ in range(num_states - 1):\n",
    "                criteria = ((train['cluster'] == s) & (train['action'] == a))\n",
    "                criteria_2 = ((train['cluster'] == s) & (train['action'] == a) & (train['cluster_n'] == s_))\n",
    "                P_t[a,s,s_] = criteria_2.sum()/criteria.sum()\n",
    "\n",
    "            P_t[a,s,num_states - 1] = 1- sum(P_t[a,s,:])\n",
    "\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            for s_ in range(num_states - 1):\n",
    "                criteria = ((ts_df['cluster'] == s) & (ts_df['action'] == a))\n",
    "                criteria_2 = ((ts_df['cluster'] == s) & (ts_df['action'] == a) & (ts_df['cluster_n'] == s_))\n",
    "                P_v[a,s,s_] = criteria_2.sum()/criteria.sum()\n",
    "            \n",
    "            P_v[a,s,num_states - 1] = 1 - sum(P_v[a,s,:])\n",
    "\n",
    "    \n",
    "    a, b = 0.01, 0.025\n",
    "    R_r = Reward(a, b,\"reward\", num_states, num_actions)\n",
    "    \n",
    "    mdp_model_r = mdptoolbox.mdp.PolicyIteration(P_t, R_r, 0.99)\n",
    "    mdp_model_r.run()\n",
    "    # if i < CV:\n",
    "    #     print(f'CV = {i+1} / Reward Model / {mdp_model_r.policy}' , end = \"\")\n",
    "    # else:\n",
    "    #     print(f'Full D / Reward Model / {mdp_model_r.policy}' , end = \"\")\n",
    "\n",
    "    R_p = Reward(a, b,\"penalty\", num_states, num_actions)\n",
    "    mdp_model_p = mdptoolbox.mdp.PolicyIteration(P_t, R_p, 0.99)\n",
    "    mdp_model_p.run()\n",
    "    #print(f' / Penalty Model / {mdp_model_p.policy}')\n",
    "\n",
    "    R_r_w = - R_r\n",
    "    mdp_model_r_w = mdptoolbox.mdp.PolicyIteration(P_t, R_r_w, 0.99)\n",
    "    mdp_model_r_w.run()\n",
    "\n",
    "    R_p_w = - R_p\n",
    "    mdp_model_p_w = mdptoolbox.mdp.PolicyIteration(P_t, R_p_w, 0.99)\n",
    "    mdp_model_p_w.run()\n",
    "\n",
    "    #policy = {\"r\":mdp_model_r.policy, \"p\":mdp_model_p.policy, \"r_w\" : mdp_model_r_w.policy, \"p_w\" : mdp_model_r_w.policy}\n",
    "    policy = {\"r\":mdp_model_r.policy, \"p\":mdp_model_p.policy, \"r_w\" : mdp_model_r_w.policy, \"p_w\" : mdp_model_r_w.policy}\n",
    "    sim_reward, random_reward, const_reward, sim_reward_w, real_reward = [0,0], [0,0], [0,0], [0,0], [0,0]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    tot_n = 0\n",
    "    for pat in valid_idx:\n",
    "        pat_d = valid[valid['PATNO'] == pat]\n",
    "        n = len(pat_d)\n",
    "        init_LEDD = list(pat_d['LEDD'])[0]\n",
    "        LEDD = {\"optimal\":[init_LEDD,init_LEDD],\n",
    "                \"random\":[init_LEDD,init_LEDD],\n",
    "                \"const\":[init_LEDD,init_LEDD],\n",
    "                \"worst\":[init_LEDD,init_LEDD],\n",
    "                \"real\":[0,0]} #sim_r, sim_p, worst_r, worst_p, constant, random\n",
    "        LEDD_traj = {\"optimal\":[init_LEDD,init_LEDD],\n",
    "                \"random\":[init_LEDD,init_LEDD],\n",
    "                \"const\":[init_LEDD,init_LEDD],\n",
    "                \"worst\":[init_LEDD,init_LEDD],\n",
    "                \"real\":[0,0]}\n",
    "\n",
    "        state = pat_d['cluster']\n",
    "        state = [list(state)[0],list(state)[0]]\n",
    "        \n",
    "        LEDD_total[\"real\"].append(pat_d[\"LEDD\"].mean())\n",
    "\n",
    "        #simul\n",
    "        for t in tests:\n",
    "            if state[0] == 0:\n",
    "                if t == \"optimal\":\n",
    "                    sim_reward[0] += 1\n",
    "                elif t == \"const\":\n",
    "                    const_reward[0] += 1\n",
    "                elif t == \"worst\":\n",
    "                    sim_reward_w[0] += 1\n",
    "                elif t == \"real\":\n",
    "                    real_reward[0] += 1\n",
    "                else:\n",
    "                    random_reward[0] += 1\n",
    "            elif state[1] == 3:\n",
    "                if t == \"optimal\":\n",
    "                    sim_reward[1] -= 1\n",
    "                elif t == \"const\":\n",
    "                    const_reward[1] -= 1\n",
    "                elif t == \"worst\":\n",
    "                    sim_reward_w[1] -= 1\n",
    "                elif t == \"real\":\n",
    "                    real_reward[1] -= 1\n",
    "                else:\n",
    "                    random_reward[1] -= 1\n",
    "\n",
    "            for nn in range(n-1):\n",
    "                if t == \"optimal\":\n",
    "                    action = [policy['r'][state[0]], policy['p'][state[1]]]\n",
    "                elif t == \"const\":\n",
    "                    action = [0,0]\n",
    "                elif t == \"worst\":\n",
    "                    action = [policy[\"r_w\"][state[0]], policy[\"p_w\"][state[1]]]\n",
    "                elif t == \"real\":\n",
    "                    action = [list(pat_d[\"action\"])[nn],list(pat_d[\"action\"])[nn]]\n",
    "                else:\n",
    "                    action = [random.choice(range(3)), random.choice(range(3))]    \n",
    "                if action[0] == 1:\n",
    "                    LEDD[t][0] += 35\n",
    "                elif action[0] == 2:\n",
    "                    LEDD[t][0] += 180\n",
    "                else:\n",
    "                    if t != \"const\":\n",
    "                        LEDD[t][0] -= 28\n",
    "\n",
    "                if action[1] == 1:\n",
    "                    LEDD[t][1] += 35\n",
    "                elif action[1] == 2:\n",
    "                    LEDD[t][1] += 180\n",
    "                else:\n",
    "                    if t != \"const\":\n",
    "                        LEDD[t][1] -= 28\n",
    "\n",
    "                for i in range(2):\n",
    "                    actions_chosen[t][i].append(action[i])\n",
    "                \n",
    "                \n",
    "                LEDD_traj[t][0] += LEDD[t][0]\n",
    "                LEDD_traj[t][1] += LEDD[t][1]\n",
    "\n",
    "                state_ = [\n",
    "                    random.choices(range(num_states), weights = P_v[int(action[0]),int(state[0]),:])[0],\n",
    "                    random.choices(range(num_states), weights = P_v[int(action[1]),int(state[1]),:])[0]\n",
    "                ]\n",
    "\n",
    "                for k in range(2):\n",
    "                    if action[k] == 1:\n",
    "                        if t == \"optimal\":\n",
    "                            sim_reward[k] -= 0.01\n",
    "                        elif t == \"const\":\n",
    "                            const_reward[k] -= 0.01\n",
    "                        elif t == \"worst\":\n",
    "                            sim_reward_w[k] -= 0.01\n",
    "                        elif t == \"real\":\n",
    "                            real_reward[k] -= 0.025\n",
    "                        else:\n",
    "                            random_reward[k] -= 0.01\n",
    "                    elif action[k] == 2:\n",
    "                        if t == \"optimal\":\n",
    "                            sim_reward[k] -= 0.025\n",
    "                        elif t == \"const\":\n",
    "                            const_reward[k] -= 0.025\n",
    "                        elif t == \"worst\":\n",
    "                            sim_reward_w[k] -= 0.025\n",
    "                        elif t == \"real\":\n",
    "                            real_reward[k] -= 0.025\n",
    "                        else:\n",
    "                            random_reward[k] -= 0.025\n",
    "                if state[0] == 0:\n",
    "                    if t == \"optimal\":\n",
    "                        sim_reward[0] += 1\n",
    "                    elif t == \"const\":\n",
    "                        const_reward[0] += 1\n",
    "                    elif t == \"worst\":\n",
    "                        sim_reward_w[0] += 1\n",
    "                    elif t == \"real\":\n",
    "                        real_reward[0] += 0.025\n",
    "                    else:\n",
    "                        random_reward[0] += 1\n",
    "                elif state[1] == 3:\n",
    "                    if t == \"optimal\":\n",
    "                        sim_reward[1] -= 1\n",
    "                    elif t == \"const\":\n",
    "                        const_reward[1] -= 1\n",
    "                    elif t == \"worst\":\n",
    "                        sim_reward_w[1] -= 1\n",
    "                    elif t == \"real\":\n",
    "                        real_reward[1] -= 1\n",
    "                    else:\n",
    "                        random_reward[1] -= 1\n",
    "                state = state_\n",
    "            \n",
    "            LEDD_traj[t][0] /= n\n",
    "            LEDD_traj[t][1] /= n\n",
    "\n",
    "            LEDD_total[t][0].append(LEDD_traj[t][0])\n",
    "            LEDD_total[t][1].append(LEDD_traj[t][1])\n",
    "\n",
    "        tot_n += n\n",
    "\n",
    "\n",
    "    sim_reward = [s/tot_n for s in sim_reward]\n",
    "    sim_reward_w = [s/tot_n for s in sim_reward_w]\n",
    "    const_reward = [s/tot_n for s in const_reward]\n",
    "    random_reward = [s/tot_n for s in random_reward]\n",
    "    real_rewards = [s/tot_n for s in real_reward]\n",
    "\n",
    "    for kk in range(2):\n",
    "        tot_sim[kk].append(sim_reward[kk])\n",
    "        tot_const[kk].append(const_reward[kk])\n",
    "        tot_random[kk].append(random_reward[kk])\n",
    "        tot_sim_w[kk].append(sim_reward_w[kk])\n",
    "        tot_real[kk].append(real_rewards[kk])\n",
    "\n",
    "tot1, tot2 = [], []\n",
    "#Real total\n",
    "\n",
    "#LEDD_total[\"real\"] = np.mean(LEDD_total[\"real\"])\n",
    "\n",
    "for t in tests:\n",
    "    LEDD_total[t][0] = np.mean(LEDD_total[t][0])\n",
    "    LEDD_total[t][1] = np.mean(LEDD_total[t][1])\n",
    "# print what kind of elements actions_chosen have copilot? \n",
    "\n",
    "for i in range(2):\n",
    "    for t in tests:\n",
    "        num_zeros = actions_chosen[t][i].count(0)\n",
    "        num_ones = actions_chosen[t][i].count(1)\n",
    "        num_twos = actions_chosen[t][i].count(2)\n",
    "        print(f\"`Actions chosen for {t}, {'reward mode' if i == 0 else 'penalty mode'} : 0s={num_zeros}, 1s={num_ones}, 2s={num_twos}`\")\n",
    "    print()\n",
    "\n",
    "modes = [\"reward\", \"penalty\"]\n",
    "for aa in range(2):\n",
    "    print(modes[aa])\n",
    "    print(\"opti\",  \"\\t\", np.round(np.mean(tot_sim[aa]),4), \"\\t\", np.round(np.std(tot_sim[aa])/sqrt(CV),4))\n",
    "    print(\"worst\", \"\\t\", np.round(np.mean(tot_sim_w[aa]),4), \"\\t\", np.round(np.std(tot_sim_w[aa])/sqrt(CV),4))\n",
    "    print(\"const\",  \"\\t\", np.round(np.mean(tot_const[aa]),4),  \"\\t\", np.round(np.std(tot_const[aa])/sqrt(CV),4))\n",
    "    print(\"random\",  \"\\t\", np.round(np.mean(tot_random[aa]),4), \"\\t\", np.round(np.std(tot_random[aa])/sqrt(CV),4))\n",
    "    print(\"real \\t\", np.round(np.mean(tot_real[aa]),4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
