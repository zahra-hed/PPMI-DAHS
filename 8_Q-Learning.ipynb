{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd0b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from math import sqrt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34b37c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read probability matrix from part 5\n",
    "ts_df = pd.read_csv(\"./dataset/generated_data/ts_df.csv\")\n",
    "ts_df = ts_df.iloc[:,1:]\n",
    "\n",
    "ts_df = ts_df.rename(columns={'action': 'action_0', 'action_2': 'action'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00bf6746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_CV(input, CV):\n",
    "    data_list = list(input)\n",
    "    random.shuffle(data_list)\n",
    "\n",
    "    subset_size = len(data_list) // CV\n",
    "    remainder = len(data_list) % CV\n",
    "\n",
    "    subsets = []\n",
    "    start_idx = 0\n",
    "    for i in range(CV):\n",
    "        end_idx = start_idx + subset_size + (1 if i < remainder else 0)\n",
    "        subsets.append(data_list[start_idx:end_idx])\n",
    "        start_idx = end_idx\n",
    "    return subsets\n",
    "\n",
    "#define the reward function\n",
    "def Reward(a, b, mode):\n",
    "    R = np.zeros(12).reshape(4,3)\n",
    "    if mode == \"reward\":\n",
    "        R[0,:] += 1\n",
    "    else:\n",
    "        R[3,:] -= 1\n",
    "    R[:,1] -= a\n",
    "    R[:,2] -= b\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75b9948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning implementation\n",
    "def q_learning(train_data, mode, n_states=4, n_actions=3, alpha=0.1, gamma=0.99, iterations=100):\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        for pat_id in train_data['PATNO'].unique():\n",
    "            pat_seq = train_data[train_data['PATNO'] == pat_id]\n",
    "            states = list(pat_seq['cluster'])\n",
    "            actions = list(pat_seq['action'])\n",
    "            next_states = list(pat_seq['cluster_n'])\n",
    "\n",
    "            for s, a, s_ in zip(states, actions, next_states):\n",
    "                if pd.isna(s_): continue\n",
    "                               \n",
    "                s = int(s)\n",
    "                a = int(a)\n",
    "                s_ = int(s_)\n",
    "                r = 0\n",
    "                if mode == \"reward\":\n",
    "                    if s_ == 0: r += 1\n",
    "                elif mode == \"penalty\":\n",
    "                    if s_ == 3: r -= 1\n",
    "\n",
    "                if a == 1: r -= 0.01\n",
    "                elif a == 2: r -= 0.025\n",
    "                \n",
    "                Q[s, a] += alpha * (r + gamma * np.max(Q[s_]) - Q[s, a])\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f050cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward\n",
      "opti \t 0.2593 \t 0.0051\n",
      "worst \t 0.2442 \t 0.005\n",
      "const \t 0.2568 \t 0.0049\n",
      "random \t 0.232 \t 0.0047\n",
      "real \t 0.0332\n",
      "penalty\n",
      "opti \t -0.1847 \t 0.0044\n",
      "worst \t -0.2053 \t 0.0066\n",
      "const \t -0.1904 \t 0.0036\n",
      "random \t -0.2102 \t 0.0051\n",
      "real \t -0.1864\n"
     ]
    }
   ],
   "source": [
    "# Simulation setup\n",
    "CV = 10\n",
    "patients = set(ts_df['PATNO'])\n",
    "subsets = divide_CV(patients, CV)\n",
    "\n",
    "tot_sim, tot_const, tot_random, tot_sim_w, tot_real = [[],[]], [[],[]], [[],[]], [[],[]], [[],[]]\n",
    "LEDD_total = {\"optimal\":[[],[]], \"random\":[[],[]], \"const\":[[],[]], \"worst\":[[],[]], \"real\":[[],[]]}\n",
    "tests = [\"optimal\", \"random\", \"const\", \"worst\", \"real\"]\n",
    "\n",
    "for i in range(CV+1):\n",
    "    if i < CV:\n",
    "        valid_idx = subsets[i]\n",
    "        train_idx = [p for j in range(CV) if j != i for p in subsets[j]]\n",
    "        valid = ts_df[ts_df['PATNO'].isin(valid_idx)]\n",
    "        train = ts_df[ts_df['PATNO'].isin(train_idx)]\n",
    "    else:\n",
    "        valid = ts_df\n",
    "        train = ts_df\n",
    "\n",
    "    # Train Q-learning policies\n",
    "    Q_r = q_learning(train, \"reward\")\n",
    "    Q_p = q_learning(train, \"penalty\")\n",
    "\n",
    "    policy = {\n",
    "        \"r\": np.argmax(Q_r, axis=1),\n",
    "        \"p\": np.argmax(Q_p, axis=1),\n",
    "        \"r_w\": np.argmin(Q_r, axis=1),\n",
    "        \"p_w\": np.argmin(Q_p, axis=1),\n",
    "    }\n",
    "\n",
    "    sim_reward, random_reward, const_reward, sim_reward_w, real_reward = [0,0], [0,0], [0,0], [0,0], [0,0]\n",
    "    tot_n = 0\n",
    "\n",
    "    for pat in valid['PATNO'].unique():\n",
    "        pat_d = valid[valid['PATNO'] == pat]\n",
    "        n = len(pat_d)\n",
    "        init_LEDD = pat_d['LEDD'].iloc[0]\n",
    "\n",
    "        LEDD = {t: [init_LEDD, init_LEDD] for t in tests}\n",
    "        LEDD_traj = {t: [init_LEDD, init_LEDD] for t in tests}\n",
    "        state = [pat_d['cluster'].iloc[0], pat_d['cluster'].iloc[0]]\n",
    "        LEDD_total[\"real\"].append(pat_d[\"LEDD\"].mean())\n",
    "\n",
    "        for t in tests:\n",
    "            if state[0] == 0:\n",
    "                if t == \"optimal\": sim_reward[0] += 1\n",
    "                elif t == \"const\": const_reward[0] += 1\n",
    "                elif t == \"worst\": sim_reward_w[0] += 1\n",
    "                elif t == \"real\": real_reward[0] += 1\n",
    "                else: random_reward[0] += 1\n",
    "            elif state[1] == 3:\n",
    "                if t == \"optimal\": sim_reward[1] -= 1\n",
    "                elif t == \"const\": const_reward[1] -= 1\n",
    "                elif t == \"worst\": sim_reward_w[1] -= 1\n",
    "                elif t == \"real\": real_reward[1] -= 1\n",
    "                else: random_reward[1] -= 1\n",
    "\n",
    "            for nn in range(n - 1):\n",
    "                if t == \"optimal\":\n",
    "                    action = [policy[\"r\"][state[0]], policy[\"p\"][state[1]]]\n",
    "                elif t == \"const\":\n",
    "                    action = [0, 0]\n",
    "                elif t == \"worst\":\n",
    "                    action = [policy[\"r_w\"][state[1]], policy[\"p_w\"][state[1]]]\n",
    "                elif t == \"real\":\n",
    "                    action = [pat_d[\"action\"].iloc[nn], pat_d[\"action\"].iloc[nn]]\n",
    "                else:\n",
    "                    action = [random.choice(range(3)), random.choice(range(3))]\n",
    "\n",
    "                for k in range(2):\n",
    "                    if action[k] == 1:\n",
    "                        LEDD[t][k] += 35\n",
    "                    elif action[k] == 2:\n",
    "                        LEDD[t][k] += 180\n",
    "                    else:\n",
    "                        if t != \"const\":\n",
    "                            LEDD[t][k] -= 28\n",
    "                    LEDD_traj[t][k] += LEDD[t][k]\n",
    "\n",
    "                state_ = [random.choice(range(4)) for _ in range(2)]  # simulate random transitions\n",
    "\n",
    "                for k in range(2):\n",
    "                    if action[k] == 1:\n",
    "                        penalty = 0.01\n",
    "                    elif action[k] == 2:\n",
    "                        penalty = 0.025\n",
    "                    else:\n",
    "                        penalty = 0\n",
    "                    if t == \"optimal\": sim_reward[k] -= penalty\n",
    "                    elif t == \"const\": const_reward[k] -= penalty\n",
    "                    elif t == \"worst\": sim_reward_w[k] -= penalty\n",
    "                    elif t == \"real\": real_reward[k] -= penalty\n",
    "                    else: random_reward[k] -= penalty\n",
    "\n",
    "                if state_[0] == 0:\n",
    "                    if t == \"optimal\": sim_reward[0] += 1\n",
    "                    elif t == \"const\": const_reward[0] += 1\n",
    "                    elif t == \"worst\": sim_reward_w[0] += 1\n",
    "                    elif t == \"real\": real_reward[0] += 0.025\n",
    "                    else: random_reward[0] += 1\n",
    "                elif state_[1] == 3:\n",
    "                    if t == \"optimal\": sim_reward[1] -= 1\n",
    "                    elif t == \"const\": const_reward[1] -= 1\n",
    "                    elif t == \"worst\": sim_reward_w[1] -= 1\n",
    "                    elif t == \"real\": real_reward[1] -= 1\n",
    "                    else: random_reward[1] -= 1\n",
    "\n",
    "                state = state_\n",
    "\n",
    "            for k in range(2):\n",
    "                LEDD_traj[t][k] /= n\n",
    "                LEDD_total[t][k].append(LEDD_traj[t][k])\n",
    "\n",
    "        tot_n += n\n",
    "\n",
    "    sim_reward = [s / tot_n for s in sim_reward]\n",
    "    sim_reward_w = [s / tot_n for s in sim_reward_w]\n",
    "    const_reward = [s / tot_n for s in const_reward]\n",
    "    random_reward = [s / tot_n for s in random_reward]\n",
    "    real_rewards = [s / tot_n for s in real_reward]\n",
    "\n",
    "    for kk in range(2):\n",
    "        tot_sim[kk].append(sim_reward[kk])\n",
    "        tot_const[kk].append(const_reward[kk])\n",
    "        tot_random[kk].append(random_reward[kk])\n",
    "        tot_sim_w[kk].append(sim_reward_w[kk])\n",
    "        tot_real[kk].append(real_rewards[kk])\n",
    "\n",
    "for t in tests:\n",
    "    LEDD_total[t][0] = np.mean(LEDD_total[t][0])\n",
    "    LEDD_total[t][1] = np.mean(LEDD_total[t][1])\n",
    "\n",
    "modes = [\"reward\", \"penalty\"]\n",
    "for aa in range(2):\n",
    "    print(modes[aa])\n",
    "    print(\"opti\",  \"\\t\", np.round(np.mean(tot_sim[aa]),4), \"\\t\", np.round(np.std(tot_sim[aa])/sqrt(CV),4))\n",
    "    print(\"worst\", \"\\t\", np.round(np.mean(tot_sim_w[aa]),4), \"\\t\", np.round(np.std(tot_sim_w[aa])/sqrt(CV),4))\n",
    "    print(\"const\",  \"\\t\", np.round(np.mean(tot_const[aa]),4),  \"\\t\", np.round(np.std(tot_const[aa])/sqrt(CV),4))\n",
    "    print(\"random\",  \"\\t\", np.round(np.mean(tot_random[aa]),4), \"\\t\", np.round(np.std(tot_random[aa])/sqrt(CV),4))\n",
    "    print(\"real \\t\", np.round(np.mean(tot_real[aa]),4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
